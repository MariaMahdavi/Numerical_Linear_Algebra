{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJh4z63fWB83"
      },
      "source": [
        "**The Steepest descent Gradient**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i7LwH5lAWHHJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def steepest_descent(f, grad_f, x0, learning_rate=0.01, tol=1e-6, max_iter=1000):\n",
        "    \"\"\"\n",
        "    Implements the steepest descent (gradient descent) method.\n",
        "\n",
        "    Parameters:\n",
        "    - f: function to minimize\n",
        "    - grad_f: gradient of the function\n",
        "    - x0: initial guess (numpy array)\n",
        "    - learning_rate: step size (float)\n",
        "    - tol: tolerance for stopping (float)\n",
        "    - max_iter: maximum number of iterations (int)\n",
        "\n",
        "    Returns:\n",
        "    - x: the point that minimizes f\n",
        "    - history: list of iterates\n",
        "    \"\"\"\n",
        "    x = x0.copy()\n",
        "    history = [x.copy()]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_f(x)\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "\n",
        "        if grad_norm < tol:\n",
        "            print(f\"Converged in {i} iterations.\")\n",
        "            break\n",
        "\n",
        "        x -= learning_rate * grad\n",
        "        history.append(x.copy())\n",
        "\n",
        "    return x, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71zFoTqBW055",
        "outputId": "9be330ff-9b8e-49f7-b85d-875f1373a564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged in 70 iterations.\n",
            "Minimum found at: [ 1.99999961 -1.9999998 ]\n",
            "Function value at minimum: -9.999999999999805\n"
          ]
        }
      ],
      "source": [
        "# Example function and gradient\n",
        "A = np.array([[3, 2], [2, 6]])\n",
        "b = np.array([2, -8])\n",
        "\n",
        "def f(x):\n",
        "    return 0.5 * x.T @ A @ x - b.T @ x\n",
        "\n",
        "def grad_f(x):\n",
        "    return A @ x - b\n",
        "\n",
        "# Initial guess\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "# Run steepest descent\n",
        "x_min, history = steepest_descent(f, grad_f, x0, learning_rate=0.1)\n",
        "\n",
        "print(\"Minimum found at:\", x_min)\n",
        "print(\"Function value at minimum:\", f(x_min))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iooDmz8gW8kt"
      },
      "source": [
        "**The conjugat Gradient method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hxOPO42IXCdM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def conjugate_gradient(A, b, x0=None, tol=1e-6, max_iter=None):\n",
        "    \"\"\"\n",
        "    Solve Ax = b using the Conjugate Gradient method.\n",
        "\n",
        "    Parameters:\n",
        "    - A: symmetric positive definite matrix (numpy array)\n",
        "    - b: right-hand side vector (numpy array)\n",
        "    - x0: initial guess (numpy array), default zeros\n",
        "    - tol: tolerance for convergence\n",
        "    - max_iter: max iterations, default size of A\n",
        "\n",
        "    Returns:\n",
        "    - x: approximate solution\n",
        "    - history: list of iterates\n",
        "    \"\"\"\n",
        "    n = b.shape[0]\n",
        "    if x0 is None:\n",
        "        x = np.zeros(n)\n",
        "    else:\n",
        "        x = x0.copy()\n",
        "\n",
        "    if max_iter is None:\n",
        "        max_iter = n\n",
        "\n",
        "    r = b - A @ x        # initial residual\n",
        "    p = r.copy()         # initial direction\n",
        "    history = [x.copy()]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        Ap = A @ p\n",
        "        r_dot = r.T @ r\n",
        "        alpha = r_dot / (p.T @ Ap)\n",
        "        x = x + alpha * p\n",
        "        r_new = r - alpha * Ap\n",
        "\n",
        "        if np.linalg.norm(r_new) < tol:\n",
        "            print(f\"Converged in {i+1} iterations.\")\n",
        "            history.append(x.copy())\n",
        "            break\n",
        "\n",
        "        beta = (r_new.T @ r_new) / r_dot\n",
        "        p = r_new + beta * p\n",
        "        r = r_new\n",
        "        history.append(x.copy())\n",
        "\n",
        "    return x, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v8S2F5xXPuP",
        "outputId": "33326122-3fef-4e55-d25a-6f843073f1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged in 2 iterations.\n",
            "Solution x: [0.09090909 0.63636364]\n",
            "Residual norm: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Example symmetric positive definite matrix A and vector b\n",
        "A = np.array([[4, 1],\n",
        "              [1, 3]])\n",
        "b = np.array([1, 2])\n",
        "\n",
        "x0 = np.zeros_like(b)\n",
        "\n",
        "x_sol, history = conjugate_gradient(A, b, x0)\n",
        "\n",
        "print(\"Solution x:\", x_sol)\n",
        "print(\"Residual norm:\", np.linalg.norm(b - A @ x_sol))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zItwKJ1XtYl"
      },
      "source": [
        "**The Natural gradient method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "glUQcFWFX2aD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def natural_gradient_descent(A, b, x0=None, learning_rate=0.1, tol=1e-6, max_iter=1000):\n",
        "    \"\"\"\n",
        "    Natural Gradient Descent for quadratic function f(x) = 1/2 x^T A x - b^T x\n",
        "    where A is symmetric positive definite matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - A: matrix (numpy array)\n",
        "    - b: vector (numpy array)\n",
        "    - x0: initial guess (numpy array)\n",
        "    - learning_rate: step size scalar\n",
        "    - tol: tolerance for stopping criterion\n",
        "    - max_iter: maximum iterations\n",
        "\n",
        "    Returns:\n",
        "    - x: solution vector\n",
        "    - history: list of iterates\n",
        "    \"\"\"\n",
        "    n = b.shape[0]\n",
        "    if x0 is None:\n",
        "        x = np.zeros(n)\n",
        "    else:\n",
        "        x = x0.copy()\n",
        "\n",
        "    # Precompute inverse of Fisher info matrix (here it's A)\n",
        "    A_inv = np.linalg.inv(A)\n",
        "    history = [x.copy()]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = A @ x - b\n",
        "        nat_grad = A_inv @ grad  # natural gradient\n",
        "\n",
        "        if np.linalg.norm(nat_grad) < tol:\n",
        "            print(f\"Converged in {i} iterations.\")\n",
        "            break\n",
        "\n",
        "        x = x - learning_rate * nat_grad\n",
        "        history.append(x.copy())\n",
        "\n",
        "    return x, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc-H-fUTX5nA",
        "outputId": "43d3cbef-94b8-42fe-d7f9-e9e16efac69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged in 127 iterations.\n",
            "Solution x: [0.09090895 0.63636265]\n",
            "Residual norm: 3.453692766043757e-06\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[4, 1],\n",
        "              [1, 3]])\n",
        "b = np.array([1, 2])\n",
        "x0 = np.zeros_like(b)\n",
        "\n",
        "x_sol, history = natural_gradient_descent(A, b, x0, learning_rate=0.1)\n",
        "\n",
        "print(\"Solution x:\", x_sol)\n",
        "print(\"Residual norm:\", np.linalg.norm(b - A @ x_sol))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
